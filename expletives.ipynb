{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\justi\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\justi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/billboard-lyrics-spotify.csv', sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decade'] = df['year'].dt.year // 10 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decade\n",
       "1960    975\n",
       "1970    986\n",
       "1980    994\n",
       "1990    933\n",
       "2000    919\n",
       "2010    709\n",
       "Name: lyrics, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = grouped_df['lyrics'].count()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_all</th>\n",
       "      <th>artist_base</th>\n",
       "      <th>rank</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist_featured</th>\n",
       "      <th>song_clean</th>\n",
       "      <th>artist_clean</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>num_words</th>\n",
       "      <th>words_per_sec</th>\n",
       "      <th>num_uniq_words</th>\n",
       "      <th>decade</th>\n",
       "      <th>uniq_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>kesha</td>\n",
       "      <td>kesha</td>\n",
       "      <td>1</td>\n",
       "      <td>tik tok</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tik tok</td>\n",
       "      <td>kesha</td>\n",
       "      <td>grab my glasses im out the door im gonna hit ...</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>120.032</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>3.328217</td>\n",
       "      <td>458.0</td>\n",
       "      <td>2.293521</td>\n",
       "      <td>136.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.367647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>2</td>\n",
       "      <td>need you now</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>need you now</td>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>picture perfect memories scattered all around...</td>\n",
       "      <td>0.09270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>107.943</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.231</td>\n",
       "      <td>4.626217</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.774571</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.792208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>hey, soul sister</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey soul sister</td>\n",
       "      <td>train</td>\n",
       "      <td>heyy heeey heey your lipstick stains on the f...</td>\n",
       "      <td>0.21700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>97.030</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.768</td>\n",
       "      <td>3.611117</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1.310767</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.469565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>katy perry  featuring  snoop dogg</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>4</td>\n",
       "      <td>california gurls</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>snoop dogg</td>\n",
       "      <td>california gurls</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>greetings loved ones lets take a journey i kn...</td>\n",
       "      <td>0.00446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>125.014</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>3.910883</td>\n",
       "      <td>445.0</td>\n",
       "      <td>1.896417</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.746914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>usher  featuring  will.i.am</td>\n",
       "      <td>usher</td>\n",
       "      <td>5</td>\n",
       "      <td>omg</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>will.i.am</td>\n",
       "      <td>omg</td>\n",
       "      <td>usher</td>\n",
       "      <td>oh my gosh baby let me i did it again so im g...</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>129.998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.326</td>\n",
       "      <td>4.491550</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1.951813</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>5.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>camila cabello  featuring  young thug</td>\n",
       "      <td>camila cabello</td>\n",
       "      <td>96</td>\n",
       "      <td>havana</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>young thug</td>\n",
       "      <td>havana</td>\n",
       "      <td>camila cabello</td>\n",
       "      <td>hey half of my heart is in havana ooh na na h...</td>\n",
       "      <td>0.18400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>104.988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.394</td>\n",
       "      <td>3.621783</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1.610625</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.431373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>maroon 5  featuring  sza</td>\n",
       "      <td>maroon 5</td>\n",
       "      <td>97</td>\n",
       "      <td>what lovers do</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sza</td>\n",
       "      <td>what lovers do</td>\n",
       "      <td>maroon 5</td>\n",
       "      <td>say say say hey hey now baby oh mama dont pla...</td>\n",
       "      <td>0.08050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>109.959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>3.330817</td>\n",
       "      <td>366.0</td>\n",
       "      <td>1.831383</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>blackbear</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>98</td>\n",
       "      <td>do re mi</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do re mi</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>do re mi fa so yeah yeah yeah oh do re mi fa ...</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>110.977</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.154</td>\n",
       "      <td>3.533783</td>\n",
       "      <td>382.0</td>\n",
       "      <td>1.801657</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.131148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>99</td>\n",
       "      <td>look at me!</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>look at me</td>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>ayy im like bitch who is your mans ayy cant k...</td>\n",
       "      <td>0.25900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>139.059</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.349</td>\n",
       "      <td>2.105767</td>\n",
       "      <td>337.0</td>\n",
       "      <td>2.667279</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.179245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>keith urban  featuring  carrie underwood</td>\n",
       "      <td>keith urban</td>\n",
       "      <td>100</td>\n",
       "      <td>the fighter</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>carrie underwood</td>\n",
       "      <td>the fighter</td>\n",
       "      <td>keith urban</td>\n",
       "      <td>i know he hurt you made you scared of love to...</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>132.023</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.761</td>\n",
       "      <td>3.067333</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2.124538</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.494253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    artist_all      artist_base  rank  \\\n",
       "4850                                     kesha            kesha     1   \n",
       "4851                           lady antebellum  lady antebellum     2   \n",
       "4852                                     train            train     3   \n",
       "4853         katy perry  featuring  snoop dogg       katy perry     4   \n",
       "4854               usher  featuring  will.i.am            usher     5   \n",
       "...                                        ...              ...   ...   \n",
       "5561     camila cabello  featuring  young thug   camila cabello    96   \n",
       "5562                  maroon 5  featuring  sza         maroon 5    97   \n",
       "5563                                 blackbear        blackbear    98   \n",
       "5564                              xxxtentacion     xxxtentacion    99   \n",
       "5565  keith urban  featuring  carrie underwood      keith urban   100   \n",
       "\n",
       "                  song       year   artist_featured        song_clean  \\\n",
       "4850           tik tok 2010-01-01               NaN           tik tok   \n",
       "4851      need you now 2010-01-01               NaN      need you now   \n",
       "4852  hey, soul sister 2010-01-01               NaN   hey soul sister   \n",
       "4853  california gurls 2010-01-01        snoop dogg  california gurls   \n",
       "4854               omg 2010-01-01         will.i.am               omg   \n",
       "...                ...        ...               ...               ...   \n",
       "5561            havana 2017-01-01        young thug            havana   \n",
       "5562    what lovers do 2017-01-01               sza    what lovers do   \n",
       "5563          do re mi 2017-01-01               NaN          do re mi   \n",
       "5564       look at me! 2017-01-01               NaN        look at me   \n",
       "5565       the fighter 2017-01-01  carrie underwood       the fighter   \n",
       "\n",
       "         artist_clean                                             lyrics  \\\n",
       "4850            kesha   grab my glasses im out the door im gonna hit ...   \n",
       "4851  lady antebellum   picture perfect memories scattered all around...   \n",
       "4852            train   heyy heeey heey your lipstick stains on the f...   \n",
       "4853       katy perry   greetings loved ones lets take a journey i kn...   \n",
       "4854            usher   oh my gosh baby let me i did it again so im g...   \n",
       "...               ...                                                ...   \n",
       "5561   camila cabello   hey half of my heart is in havana ooh na na h...   \n",
       "5562         maroon 5   say say say hey hey now baby oh mama dont pla...   \n",
       "5563        blackbear   do re mi fa so yeah yeah yeah oh do re mi fa ...   \n",
       "5564     xxxtentacion   ayy im like bitch who is your mans ayy cant k...   \n",
       "5565      keith urban   i know he hurt you made you scared of love to...   \n",
       "\n",
       "      acousticness  ...  speechiness    tempo  time_signature  valence  \\\n",
       "4850       0.07460  ...       0.1160  120.032             4.0    0.735   \n",
       "4851       0.09270  ...       0.0303  107.943             4.0    0.231   \n",
       "4852       0.21700  ...       0.0436   97.030             4.0    0.768   \n",
       "4853       0.00446  ...       0.0569  125.014             4.0    0.425   \n",
       "4854       0.19800  ...       0.0332  129.998             4.0    0.326   \n",
       "...            ...  ...          ...      ...             ...      ...   \n",
       "5561       0.18400  ...       0.0300  104.988             4.0    0.394   \n",
       "5562       0.08050  ...       0.0693  109.959             4.0    0.420   \n",
       "5563       0.00419  ...       0.0465  110.977             3.0    0.154   \n",
       "5564       0.25900  ...       0.2820  139.059             4.0    0.349   \n",
       "5565       0.03040  ...       0.0525  132.023             4.0    0.761   \n",
       "\n",
       "      duration_min  num_words  words_per_sec  num_uniq_words  decade  \\\n",
       "4850      3.328217      458.0       2.293521           136.0    2010   \n",
       "4851      4.626217      215.0       0.774571            77.0    2010   \n",
       "4852      3.611117      284.0       1.310767           115.0    2010   \n",
       "4853      3.910883      445.0       1.896417           162.0    2010   \n",
       "4854      4.491550      526.0       1.951813           105.0    2010   \n",
       "...            ...        ...            ...             ...     ...   \n",
       "5561      3.621783      350.0       1.610625           102.0    2010   \n",
       "5562      3.330817      366.0       1.831383            75.0    2010   \n",
       "5563      3.533783      382.0       1.801657           122.0    2010   \n",
       "5564      2.105767      337.0       2.667279           106.0    2010   \n",
       "5565      3.067333      391.0       2.124538            87.0    2010   \n",
       "\n",
       "     uniq_ratio  \n",
       "4850   3.367647  \n",
       "4851   2.792208  \n",
       "4852   2.469565  \n",
       "4853   2.746914  \n",
       "4854   5.009524  \n",
       "...         ...  \n",
       "5561   3.431373  \n",
       "5562   4.880000  \n",
       "5563   3.131148  \n",
       "5564   3.179245  \n",
       "5565   4.494253  \n",
       "\n",
       "[716 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.get_group(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1960, 1970, 1980, 1990, 2000, 2010])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' do re mi fa so yeah yeah yeah oh do re mi fa so yeah yeah yeah yeah if i could go back to the day we met i probably would just stay in bed you run your mouth all over town and this one goes out to the sound of breaking glass on my range rover pay me back or bitch its over all the presents i would send fuck my friends behind my shoulder next time ima stay asleep i pray the lord my soul to keep oh and you got me thinking lately bitch you crazy and nothings ever good enough i wrote a little song for ya it go like do re mi fa so fuckin done with you girl so fuckin done with all the games you play i aint no tic tac toe send the x and os on another note im do re mi fa so fuckin done with you baby so send the x and os on another note im ghost if i could go back to the day we met i probably wouldve stayed in bed you wake up everyday and make me feel like im incompetent designer shoes and xanax tabs compliments your make up bag you never had to buy yourself a drink cause everybody want to tap that ass sometime and you got me thinking lately bitch you crazy and nothings ever good enough i wrote a little song for ya it go like do re mi fa so fuckin done with you girl so fuckin done with all the games you play i aint no tic tac toe send the x and os on another note im do re mi fa so fuckin done with you baby so send the x and os on another note im ghost i wrote a little song for you it go like do re mi fa so fuckin done with you girl so fuckin done with all the games you play i aint no tic tac toe send the x and os on another note im do re mi fa so fuckin done with you baby so send the x and os on another note im ghost do re mi fa so so send the x and os on another note im ghost'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['song'] == 'do re mi']['lyrics'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lyrics']\n",
    "lyrics = [item for item in df['lyrics']]\n",
    "lyrics_strings = list(map(str, lyrics))\n",
    "lyrics_words = [word_tokenize(item) for item in lyrics_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per song: 303.3695652173913\n",
      "Minimum number of words per song: 0\n",
      "Maximum number of words per song: 1153\n"
     ]
    }
   ],
   "source": [
    "word_counts = [len(item) for item in lyrics_words]\n",
    "avg_word_counts = np.mean(word_counts)\n",
    "min_words = np.min(word_counts)\n",
    "max_words = np.max(word_counts)\n",
    "print('Average number of words per song: {}'.format(avg_word_counts))\n",
    "print('Minimum number of words per song: {}'.format(min_words))\n",
    "print('Maximum number of words per song: {}'.format(max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "lowercased = [[word.lower() for word in item] for item in lyrics_words]\n",
    "removed_punctuation = [[word for word in review if word.isalnum()] for review in lowercased]\n",
    "removed_stopwords = [[word for word in review if word not in sw] for review in removed_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed = [[ps.stem(word) for word in review] for review in removed_stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "unigrams = [list(ngrams(item, 1)) for item in removed_punctuation]\n",
    "bigrams = [list(ngrams(item, 2)) for item in removed_punctuation]\n",
    "trigrams = [list(ngrams(item, 3)) for item in removed_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9f13933975489ebea490da9a5d7d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import BERT model\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# load tokenizer (same one trained with the model)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "# load pre-trained BERT model weights\n",
    "model = BertModel.from_pretrained(\n",
    "    \"unitary/toxic-bert\",\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    ")\n",
    "# model = torch.hub.load('unitaryai/detoxify','multilingual_toxic_xlm_r')\n",
    "# model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2079,  2128,  2771,  6904,  2061,  3398,  3398,  3398,  2821,\n",
       "          2079,  2128,  2771,  6904,  2061,  3398,  3398,  3398,  3398,  2065,\n",
       "          1045,  2071,  2175,  2067,  2000,  1996,  2154,  2057,  2777,  1045,\n",
       "          2763,  2052,  2074,  2994,  1999,  2793,  2017,  2448,  2115,  2677,\n",
       "          2035,  2058,  2237,  1998,  2023,  2028,  3632,  2041,  2000,  1996,\n",
       "          2614,  1997,  4911,  3221,  2006,  2026,  2846, 13631,  3477,  2033,\n",
       "          2067,  2030,  7743,  2049,  2058,  2035,  1996,  7534,  1045,  2052,\n",
       "          4604,  6616,  2026,  2814,  2369,  2026,  3244,  2279,  2051, 10047,\n",
       "          2050,  2994,  6680,  1045, 11839,  1996,  2935,  2026,  3969,  2000,\n",
       "          2562,  2821,  1998,  2017,  2288,  2033,  3241,  9906,  7743,  2017,\n",
       "          4689,  1998,  2498,  2015,  2412,  2204,  2438,  1045,  2626,  1037,\n",
       "          2210,  2299,  2005,  8038,  2009,  2175,  2066,  2079,  2128,  2771,\n",
       "          6904,  2061,  6616,  2378,  2589,  2007,  2017,  2611,  2061,  6616,\n",
       "          2378,  2589,  2007,  2035,  1996,  2399,  2017,  2377,  1045,  7110,\n",
       "          2102,  2053, 14841,  2278, 11937,  2278, 11756,  4604,  1996,  1060,\n",
       "          1998,  9808,  2006,  2178,  3602, 10047,  2079,  2128,  2771,  6904,\n",
       "          2061,  6616,  2378,  2589,  2007,  2017,  3336,  2061,  4604,  1996,\n",
       "          1060,  1998,  9808,  2006,  2178,  3602, 10047,  5745,  2065,  1045,\n",
       "          2071,  2175,  2067,  2000,  1996,  2154,  2057,  2777,  1045,  2763,\n",
       "          2052,  3726,  4370,  1999,  2793,  2017,  5256,  2039, 10126,  1998,\n",
       "          2191,  2033,  2514,  2066, 10047,  4297, 25377, 12870,  3372,  5859,\n",
       "          6007,  1998,  1060,  5162,  2595, 21628,  2015, 19394,  2015,  2115,\n",
       "          2191,  2039,  4524,  2017,  2196,  2018,  2000,  4965,  4426,  1037,\n",
       "          4392,  3426,  7955,  2215,  2000, 11112,  2008,  4632,  8811,  1998,\n",
       "          2017,  2288,  2033,  3241,  9906,  7743,  2017,  4689,  1998,  2498,\n",
       "          2015,  2412,  2204,  2438,  1045,  2626,  1037,  2210,  2299,  2005,\n",
       "          8038,  2009,  2175,  2066,  2079,  2128,  2771,  6904,  2061,  6616,\n",
       "          2378,  2589,  2007,  2017,  2611,  2061,  6616,  2378,  2589,  2007,\n",
       "          2035,  1996,  2399,  2017,  2377,  1045,  7110,  2102,  2053, 14841,\n",
       "          2278, 11937,  2278, 11756,  4604,  1996,  1060,  1998,  9808,  2006,\n",
       "          2178,  3602, 10047,  2079,  2128,  2771,  6904,  2061,  6616,  2378,\n",
       "          2589,  2007,  2017,  3336,  2061,  4604,  1996,  1060,  1998,  9808,\n",
       "          2006,  2178,  3602, 10047,  5745,  1045,  2626,  1037,  2210,  2299,\n",
       "          2005,  2017,  2009,  2175,  2066,  2079,  2128,  2771,  6904,  2061,\n",
       "          6616,  2378,  2589,  2007,  2017,  2611,  2061,  6616,  2378,  2589,\n",
       "          2007,  2035,  1996,  2399,  2017,  2377,  1045,  7110,  2102,  2053,\n",
       "         14841,  2278, 11937,  2278, 11756,  4604,  1996,  1060,  1998,  9808,\n",
       "          2006,  2178,  3602, 10047,  2079,  2128,  2771,  6904,  2061,  6616,\n",
       "          2378,  2589,  2007,  2017,  3336,  2061,  4604,  1996,  1060,  1998,\n",
       "          9808,  2006,  2178,  3602, 10047,  5745,  2079,  2128,  2771,  6904,\n",
       "          2061,  2061,  4604,  1996,  1060,  1998,  9808,  2006,  2178,  3602,\n",
       "         10047,  5745,   102]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = df[df['song'] == 'do re mi']['lyrics'].values[0]\n",
    "input1 = torch.tensor([tokenizer.encode(e1, add_special_tokens=True)])\n",
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justi\\GitHub\\info498b_lyrics_analysis\\expletives.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/justi/GitHub/info498b_lyrics_analysis/expletives.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, token_str \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokenizer\u001b[39m.\u001b[39mtokenize(e1)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justi/GitHub/info498b_lyrics_analysis/expletives.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(i, token_str)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justi/GitHub/info498b_lyrics_analysis/expletives.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenizer.tokenize(e1)):\n",
    "    print(i, token_str)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input1)\n",
    "    hidden1 = output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0\n",
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 413\n",
      "Number of hidden units: 768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden = [hidden1]\n",
    "for i in range(len(hidden)):\n",
    "    print(f'Sentence {i}')\n",
    "    print (\"Number of layers:\", len(hidden_i), \"  (initial embeddings + 12 BERT layers)\")\n",
    "    print (\"Number of batches:\", len(hidden_i[0]))\n",
    "    print (\"Number of tokens:\", len(hidden_i[0][0]))\n",
    "    print (\"Number of hidden units:\", len(hidden_i[0][0][0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([413, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = []\n",
    "\n",
    "for i in range(len(hidden)):\n",
    "    hidden_i = hidden[i]\n",
    "    token_embeddings_i = torch.stack(hidden_i, dim=0)\n",
    "    token_embeddings_i = torch.squeeze(token_embeddings_i, dim=1)\n",
    "    token_embeddings_i = token_embeddings_i.permute(1,0,2)\n",
    "    print(token_embeddings_i.size())\n",
    "    token_embeddings.append(token_embeddings_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 1 x 413\n"
     ]
    }
   ],
   "source": [
    "idx1 = 7\n",
    "\n",
    "token_vecs_cat = []\n",
    "for i, doc_rep in enumerate(token_embeddings):\n",
    "    token_Vec = []\n",
    "    for token_rep in doc_rep:\n",
    "        cat_vec = torch.cat((token_rep[-1], token_rep[-2], token_rep[-3], token_rep[-4]), dim=0)\n",
    "        token_Vec.append(cat_vec)\n",
    "    token_vecs_cat.append(token_Vec)\n",
    "\n",
    "print('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "\n",
    "words = []\n",
    "words.append(token_vecs_cat[0][idx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_embedding = []\n",
    "\n",
    "for i, doc in enumerate(token_vecs_cat):\n",
    "    sentence_embedding_i = torch.mean(torch.stack(doc), dim=0)\n",
    "    sentence_embedding.append(sentence_embedding_i)\n",
    "\n",
    "cos_s1_s2 = 1 - cosine(sentence_embedding[0], sentence_embedding[1])\n",
    "\n",
    "print(\"s1: \", e1)\n",
    "print()\n",
    "print('Vector similarity for ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info498b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
