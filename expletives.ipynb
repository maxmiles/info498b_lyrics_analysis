{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\justi\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\justi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\justi\\miniconda3\\envs\\info498b\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/billboard-lyrics-spotify.csv', sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decade'] = df['year'].dt.year // 10 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['lyrics'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decade\n",
       "1960    975\n",
       "1970    986\n",
       "1980    994\n",
       "1990    933\n",
       "2000    919\n",
       "2010    709\n",
       "Name: lyrics, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = grouped_df['lyrics'].count()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_all</th>\n",
       "      <th>artist_base</th>\n",
       "      <th>rank</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist_featured</th>\n",
       "      <th>song_clean</th>\n",
       "      <th>artist_clean</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>num_words</th>\n",
       "      <th>words_per_sec</th>\n",
       "      <th>num_uniq_words</th>\n",
       "      <th>decade</th>\n",
       "      <th>uniq_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>kesha</td>\n",
       "      <td>kesha</td>\n",
       "      <td>1</td>\n",
       "      <td>tik tok</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tik tok</td>\n",
       "      <td>kesha</td>\n",
       "      <td>grab my glasses im out the door im gonna hit ...</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>120.032</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>3.328217</td>\n",
       "      <td>458.0</td>\n",
       "      <td>2.293521</td>\n",
       "      <td>136.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.367647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>2</td>\n",
       "      <td>need you now</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>need you now</td>\n",
       "      <td>lady antebellum</td>\n",
       "      <td>picture perfect memories scattered all around...</td>\n",
       "      <td>0.09270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>107.943</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.231</td>\n",
       "      <td>4.626217</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.774571</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.792208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>hey, soul sister</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey soul sister</td>\n",
       "      <td>train</td>\n",
       "      <td>heyy heeey heey your lipstick stains on the f...</td>\n",
       "      <td>0.21700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>97.030</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.768</td>\n",
       "      <td>3.611117</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1.310767</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.469565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4853</th>\n",
       "      <td>katy perry  featuring  snoop dogg</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>4</td>\n",
       "      <td>california gurls</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>snoop dogg</td>\n",
       "      <td>california gurls</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>greetings loved ones lets take a journey i kn...</td>\n",
       "      <td>0.00446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0569</td>\n",
       "      <td>125.014</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.425</td>\n",
       "      <td>3.910883</td>\n",
       "      <td>445.0</td>\n",
       "      <td>1.896417</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.746914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>usher  featuring  will.i.am</td>\n",
       "      <td>usher</td>\n",
       "      <td>5</td>\n",
       "      <td>omg</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>will.i.am</td>\n",
       "      <td>omg</td>\n",
       "      <td>usher</td>\n",
       "      <td>oh my gosh baby let me i did it again so im g...</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>129.998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.326</td>\n",
       "      <td>4.491550</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1.951813</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>5.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>camila cabello  featuring  young thug</td>\n",
       "      <td>camila cabello</td>\n",
       "      <td>96</td>\n",
       "      <td>havana</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>young thug</td>\n",
       "      <td>havana</td>\n",
       "      <td>camila cabello</td>\n",
       "      <td>hey half of my heart is in havana ooh na na h...</td>\n",
       "      <td>0.18400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>104.988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.394</td>\n",
       "      <td>3.621783</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1.610625</td>\n",
       "      <td>102.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.431373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>maroon 5  featuring  sza</td>\n",
       "      <td>maroon 5</td>\n",
       "      <td>97</td>\n",
       "      <td>what lovers do</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>sza</td>\n",
       "      <td>what lovers do</td>\n",
       "      <td>maroon 5</td>\n",
       "      <td>say say say hey hey now baby oh mama dont pla...</td>\n",
       "      <td>0.08050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>109.959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>3.330817</td>\n",
       "      <td>366.0</td>\n",
       "      <td>1.831383</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>blackbear</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>98</td>\n",
       "      <td>do re mi</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do re mi</td>\n",
       "      <td>blackbear</td>\n",
       "      <td>do re mi fa so yeah yeah yeah oh do re mi fa ...</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>110.977</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.154</td>\n",
       "      <td>3.533783</td>\n",
       "      <td>382.0</td>\n",
       "      <td>1.801657</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.131148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>99</td>\n",
       "      <td>look at me!</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>look at me</td>\n",
       "      <td>xxxtentacion</td>\n",
       "      <td>ayy im like bitch who is your mans ayy cant k...</td>\n",
       "      <td>0.25900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>139.059</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.349</td>\n",
       "      <td>2.105767</td>\n",
       "      <td>337.0</td>\n",
       "      <td>2.667279</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.179245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>keith urban  featuring  carrie underwood</td>\n",
       "      <td>keith urban</td>\n",
       "      <td>100</td>\n",
       "      <td>the fighter</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>carrie underwood</td>\n",
       "      <td>the fighter</td>\n",
       "      <td>keith urban</td>\n",
       "      <td>i know he hurt you made you scared of love to...</td>\n",
       "      <td>0.03040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>132.023</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.761</td>\n",
       "      <td>3.067333</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2.124538</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>4.494253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    artist_all      artist_base  rank  \\\n",
       "4850                                     kesha            kesha     1   \n",
       "4851                           lady antebellum  lady antebellum     2   \n",
       "4852                                     train            train     3   \n",
       "4853         katy perry  featuring  snoop dogg       katy perry     4   \n",
       "4854               usher  featuring  will.i.am            usher     5   \n",
       "...                                        ...              ...   ...   \n",
       "5561     camila cabello  featuring  young thug   camila cabello    96   \n",
       "5562                  maroon 5  featuring  sza         maroon 5    97   \n",
       "5563                                 blackbear        blackbear    98   \n",
       "5564                              xxxtentacion     xxxtentacion    99   \n",
       "5565  keith urban  featuring  carrie underwood      keith urban   100   \n",
       "\n",
       "                  song       year   artist_featured        song_clean  \\\n",
       "4850           tik tok 2010-01-01               NaN           tik tok   \n",
       "4851      need you now 2010-01-01               NaN      need you now   \n",
       "4852  hey, soul sister 2010-01-01               NaN   hey soul sister   \n",
       "4853  california gurls 2010-01-01        snoop dogg  california gurls   \n",
       "4854               omg 2010-01-01         will.i.am               omg   \n",
       "...                ...        ...               ...               ...   \n",
       "5561            havana 2017-01-01        young thug            havana   \n",
       "5562    what lovers do 2017-01-01               sza    what lovers do   \n",
       "5563          do re mi 2017-01-01               NaN          do re mi   \n",
       "5564       look at me! 2017-01-01               NaN        look at me   \n",
       "5565       the fighter 2017-01-01  carrie underwood       the fighter   \n",
       "\n",
       "         artist_clean                                             lyrics  \\\n",
       "4850            kesha   grab my glasses im out the door im gonna hit ...   \n",
       "4851  lady antebellum   picture perfect memories scattered all around...   \n",
       "4852            train   heyy heeey heey your lipstick stains on the f...   \n",
       "4853       katy perry   greetings loved ones lets take a journey i kn...   \n",
       "4854            usher   oh my gosh baby let me i did it again so im g...   \n",
       "...               ...                                                ...   \n",
       "5561   camila cabello   hey half of my heart is in havana ooh na na h...   \n",
       "5562         maroon 5   say say say hey hey now baby oh mama dont pla...   \n",
       "5563        blackbear   do re mi fa so yeah yeah yeah oh do re mi fa ...   \n",
       "5564     xxxtentacion   ayy im like bitch who is your mans ayy cant k...   \n",
       "5565      keith urban   i know he hurt you made you scared of love to...   \n",
       "\n",
       "      acousticness  ...  speechiness    tempo  time_signature  valence  \\\n",
       "4850       0.07460  ...       0.1160  120.032             4.0    0.735   \n",
       "4851       0.09270  ...       0.0303  107.943             4.0    0.231   \n",
       "4852       0.21700  ...       0.0436   97.030             4.0    0.768   \n",
       "4853       0.00446  ...       0.0569  125.014             4.0    0.425   \n",
       "4854       0.19800  ...       0.0332  129.998             4.0    0.326   \n",
       "...            ...  ...          ...      ...             ...      ...   \n",
       "5561       0.18400  ...       0.0300  104.988             4.0    0.394   \n",
       "5562       0.08050  ...       0.0693  109.959             4.0    0.420   \n",
       "5563       0.00419  ...       0.0465  110.977             3.0    0.154   \n",
       "5564       0.25900  ...       0.2820  139.059             4.0    0.349   \n",
       "5565       0.03040  ...       0.0525  132.023             4.0    0.761   \n",
       "\n",
       "      duration_min  num_words  words_per_sec  num_uniq_words  decade  \\\n",
       "4850      3.328217      458.0       2.293521           136.0    2010   \n",
       "4851      4.626217      215.0       0.774571            77.0    2010   \n",
       "4852      3.611117      284.0       1.310767           115.0    2010   \n",
       "4853      3.910883      445.0       1.896417           162.0    2010   \n",
       "4854      4.491550      526.0       1.951813           105.0    2010   \n",
       "...            ...        ...            ...             ...     ...   \n",
       "5561      3.621783      350.0       1.610625           102.0    2010   \n",
       "5562      3.330817      366.0       1.831383            75.0    2010   \n",
       "5563      3.533783      382.0       1.801657           122.0    2010   \n",
       "5564      2.105767      337.0       2.667279           106.0    2010   \n",
       "5565      3.067333      391.0       2.124538            87.0    2010   \n",
       "\n",
       "     uniq_ratio  \n",
       "4850   3.367647  \n",
       "4851   2.792208  \n",
       "4852   2.469565  \n",
       "4853   2.746914  \n",
       "4854   5.009524  \n",
       "...         ...  \n",
       "5561   3.431373  \n",
       "5562   4.880000  \n",
       "5563   3.131148  \n",
       "5564   3.179245  \n",
       "5565   4.494253  \n",
       "\n",
       "[709 rows x 30 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.get_group(2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['lyrics'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_lyrics = {}\n",
    "for decade, group in grouped_df:\n",
    "    lyrics = [item for item in group['lyrics']]\n",
    "    grouped_lyrics[decade] = lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['lyrics']\n",
    "lyrics = [item for item in df['lyrics']]\n",
    "lyrics_strings = list(map(str, lyrics))\n",
    "lyrics_words = [word_tokenize(item) for item in lyrics_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per song: 306.1104060913706\n",
      "Minimum number of words per song: 0\n",
      "Maximum number of words per song: 1153\n"
     ]
    }
   ],
   "source": [
    "word_counts = [len(item) for item in lyrics_words]\n",
    "avg_word_counts = np.mean(word_counts)\n",
    "min_words = np.min(word_counts)\n",
    "max_words = np.max(word_counts)\n",
    "print('Average number of words per song: {}'.format(avg_word_counts))\n",
    "print('Minimum number of words per song: {}'.format(min_words))\n",
    "print('Maximum number of words per song: {}'.format(max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "lowercased = [[word.lower() for word in item] for item in lyrics_words]\n",
    "removed_punctuation = [[word for word in review if word.isalnum()] for review in lowercased]\n",
    "removed_stopwords = [[word for word in review if word not in sw] for review in removed_punctuation]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed = [[ps.stem(word) for word in review] for review in removed_stopwords]\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "unigrams = [list(ngrams(item, 1)) for item in removed_punctuation]\n",
    "bigrams = [list(ngrams(item, 2)) for item in removed_punctuation]\n",
    "trigrams = [list(ngrams(item, 3)) for item in removed_punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6599a55f524ab4b2fb9c9f832049a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1491a515db448bb9305aaaf3aa4316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f49971c8e5430ea300232700e83f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd705d7f348f447ba2c892651b963639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456096a5492413e92ba645a384b59a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c699366f1324c198a90fc394c1ebf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13541921817f4cad96cce50de41e5a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\justi/.cache\\torch\\sentence_transformers\\unitary_toxic-bert. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# import BERT model\n",
    "\n",
    "# load tokenizer (same one trained with the model)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "# load pre-trained BERT model weights\n",
    "# model = BertModel.from_pretrained(\n",
    "#     \"unitary/toxic-bert\",\n",
    "#     output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "model = SentenceTransformer('unitary/toxic-bert')\n",
    "\n",
    "# model = torch.hub.load('unitaryai/detoxify','multilingual_toxic_xlm_r')\n",
    "# model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_embeddings = {}\n",
    "\n",
    "for decade in grouped_lyrics:\n",
    "    # decade_embeddings[decade] = np.mean(group['embedding'].values, axis=0)\n",
    "    # print(decade, group['lyrics'])\n",
    "    # print(len(group['lyrics']))\n",
    "    songs = grouped_lyrics[decade]\n",
    "    decade_embeddings[decade] = model.encode(songs, convert_to_tensor=True)\n",
    "    # for lyrics in group['lyrics']:\n",
    "    #     tokens = tokenizer(lyrics, return_tensors='pt')\n",
    "     \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_mean = {}\n",
    "for decade in decade_embeddings:\n",
    "    decade_mean[decade] = torch.mean(decade_embeddings[decade], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(decade_mean.values())\n",
    "array_decade_mean = torch.stack(list(decade_mean.values()))\n",
    "np_means = array_decade_mean.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9991, 0.9980, 0.9755, 0.9149, 0.8693],\n",
       "        [0.9991, 1.0000, 0.9984, 0.9793, 0.9237, 0.8792],\n",
       "        [0.9980, 0.9984, 1.0000, 0.9817, 0.9230, 0.8815],\n",
       "        [0.9755, 0.9793, 0.9817, 1.0000, 0.9769, 0.9530],\n",
       "        [0.9149, 0.9237, 0.9230, 0.9769, 1.0000, 0.9921],\n",
       "        [0.8693, 0.8792, 0.8815, 0.9530, 0.9921, 1.0000]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim = util.cos_sim(np_means, np_means)\n",
    "cossim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info498b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
